# -*- coding: utf-8 -*-
"""Data_wrangling_GtX_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-I_CHJtfX0DCZ1LtEzyOOJlox9Qb2OCk

Import Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
#progress bar
from tqdm.notebook import tqdm

""" Load data from google drive"""

def folder_download(folder_id):
  # authenticate
  from google.colab import auth
  auth.authenticate_user()
  # get folder_name
  from googleapiclient.discovery import build
  service = build('drive', 'v3')
  folder_name = service.files().get(fileId=folder_id).execute()['name']
  # import library and download
  !wget -qnc https://github.com/segnolin/google-drive-folder-downloader/raw/master/download.py
  from download import download_folder
  download_folder(service, folder_id, './', folder_name)
  return folder_name

folder_download('1g58xFQ9aDcTPetma4blzVTQYX-QKkzNf')

# unzip some files
!mkdir clean_las_files
! cd Data\ for\ Datathon && unzip well_log_files.zip -d /content/clean_las_files

# set paths of different subdirectories
dir_eaglebine = 'Data for Datathon/Eaglebine/Eaglebine/'
dir_duvernay = 'Data for Datathon/Duvernay/Duvernay/'

"""# read temperature data for Eaglebine field
"""

Eaglebine_BHT = pd.read_excel(dir_eaglebine+'Eaglebine BHT TSC data for SPE April 21 2020.xlsx')
Eaglebine_BHT['TempC_BHT'] = (Eaglebine_BHT['BHTorMRT (maximum recorded temperature) oF'] - 32) * (5./9.)
# Eaglebine_BHT.head()
Eaglebine_BHT.to_csv('Eaglebine.csv', index=False)
print("number of unique wells: " + str(len(pd.unique(Eaglebine_BHT['UWI']))))

"""## Read in Eaglebine synthetic "true formation temperature""""

Eaglebine_Truth = pd.read_excel(dir_eaglebine+'Eaglebine TrueTemp_Train2.xlsx')
Eaglebine_Truth.head()

# convert to Celsius
Eaglebine_Truth['TempTrue_degC'] = (Eaglebine_Truth['True Temperature   (oF)'] - 32) * (5./9.)

wells = pd.unique(Eaglebine_Truth["UWI"])
UWI = wells[7]

# plot geothermal gradient
def plot_geothermal_gradient(x):
  well_temp_depth_data = Eaglebine_Truth.loc[UWI == Eaglebine_Truth["UWI"], ["Depth sub-sea (feet)", "True Temperature   (oF)"]]
  plt.plot(well_temp_depth_data["True Temperature   (oF)"].values, well_temp_depth_data["Depth sub-sea (feet)"].values * -1 )
  plt.xlabel("Temperature")
  plt.ylabel("Depth")
  plt.show()

plot_geothermal_gradient(UWI)

print("number of unique wells in Eaglebine in training set: " + str(len(pd.unique(Eaglebine_Truth['UWI']))))

"""## Combine measured BHT and true temperature"""

Eaglebine_Combined = Eaglebine_BHT.merge(Eaglebine_Truth, on='UWI', how='left')
# Eaglebine_Combined.head()

"""# Feature Engineering (interpolate true temperatures at bottomhole depth)"""

def predict_well_temp(dataset):
  scores = []
  df_all_wells = []
  wells = pd.unique(dataset["UWI"])
  for well in wells:
    well_temp_depth_data = dataset.loc[well == dataset["UWI"], dataset.columns]
    if len(well_temp_depth_data) == 1:
      df = pd.DataFrame(data=None, columns = ["UWI", "True Bottomhole Depth (ft)", "New_Bottomhole_TempTrue_degC"])
      df["True Bottomhole Depth (ft)"] = dataset.loc[well == dataset["UWI"], "BHT_below sea level (ft)"]
      df["UWI"] = well
      df["New_Bottomhole_TempTrue_degC"] = np.nan
      scores.append(np.nan)
    else:
      # train
      X_train, X_test, y_train, y_test = train_test_split(well_temp_depth_data["Depth sub-sea (feet)"].values, well_temp_depth_data["TempTrue_degC"].values, test_size=0.25, random_state=42)
      regr = RandomForestRegressor(n_estimators=10)
      regr.fit(X_train.reshape(-1, 1), y_train)
      scores.append(regr.score(X_test.reshape(-1, 1), y_test))

      # predict
      X_new = dataset.loc[well == dataset["UWI"], "BHT_below sea level (ft)"].values
      new_temps = regr.predict(X_new.reshape(-1, 1))
      df = pd.DataFrame(data=None, columns = ["UWI", "True Bottomhole Depth (ft)", "New_Bottomhole_TempTrue_degC"])
      df["True Bottomhole Depth (ft)"] = dataset.loc[well == dataset["UWI"], "BHT_below sea level (ft)"]
      df["UWI"] = well
      df["New_Bottomhole_TempTrue_degC"] = new_temps
    df_all_wells.append(df)

  df_all_wells_ = pd.concat(df_all_wells)
  return df_all_wells_, scores

Eaglebine_new, model_scores = predict_well_temp(Eaglebine_Combined)
Eaglebine_Combined_new = pd.merge(Eaglebine_Combined, Eaglebine_new, left_index=True, right_index=True)
Eaglebine_Combined_new = Eaglebine_Combined_new.drop(["UWI_y", "True Bottomhole Depth (ft)"], axis=1)
Eaglebine_Combined_new = Eaglebine_Combined_new.rename(columns={"UWI_x": "UWI"})
# Eaglebine_Combined_new.head()

# only keep from the synthetic data, the temperature at the elevation closest to the model
Eaglebine_Combined_new['diff_depth'] = Eaglebine_Combined_new['Depth sub-sea (feet)']-Eaglebine_Combined_new['BHT_below sea level (ft)']
Eaglebine_Combined_new['diff_depth_abs'] = np.abs(Eaglebine_Combined_new['diff_depth'])
idx = Eaglebine_Combined_new.groupby(['UWI'])['diff_depth_abs'].transform(min) == Eaglebine_Combined_new['diff_depth_abs']
TrueTempUWI = Eaglebine_Combined_new.loc[idx, ['UWI', 'diff_depth_abs', 'TempTrue_degC', 'New_Bottomhole_TempTrue_degC']]
TrueTempUWI = TrueTempUWI.copy(deep=True)
Eaglebine_Combined_new_cln = Eaglebine_BHT.merge(TrueTempUWI, on='UWI', how='left')
# Eaglebine_Combined_new_cln.head()

# len(Eaglebine_Combined_new_cln)

"""## Read in static temperature information and merge into dataset"""

Static_log_temp = pd.read_csv('Data for Datathon/Data_static_logs.csv')
# Static_log_temp.head()
# len(Static_log_temp)

Eaglebine_Combined_new_cln['UWI'] = Eaglebine_Combined_new_cln['UWI'].astype(str)
Eaglebine_Combined_new_cln = Eaglebine_Combined_new_cln.copy(deep=True)
Eaglebine_Combined_new_cln['TrueTemp_datasource_syn'] = 'synthetic'
Static_log_temp['TrueTemp_datasource_stat'] = 'static_temp_logs'
Eaglebine_Combined_stat = Eaglebine_Combined_new_cln.merge(Static_log_temp, left_on='UWI',right_on='Well_ID', how='left')

# Coalesce columns together with priority for true temperature measurements
Eaglebine_Combined_stat['TempC_Fin_provided'] = Eaglebine_Combined_stat['Temp (degC)'].fillna(Eaglebine_Combined_stat['TempTrue_degC'])
Eaglebine_Combined_stat['TempC_Fin_extrapolated'] = Eaglebine_Combined_stat['Temp (degC)'].fillna(Eaglebine_Combined_stat['New_Bottomhole_TempTrue_degC'])
Eaglebine_Combined_stat['TrueTemp_datasource'] = Eaglebine_Combined_stat['TrueTemp_datasource_stat'].fillna(Eaglebine_Combined_stat['TrueTemp_datasource_syn'])
# Eaglebine_Combined_stat.head()

"""## plot Eaglebine temperature data"""

import matplotlib.pyplot as plt
plt.close('all')
fig, ax = plt.subplots(1, 1, figsize=(4,4))

sns.scatterplot(data=Eaglebine_Combined_stat, x="TempC_BHT", y="TempC_Fin_extrapolated", hue='BHT_below sea level (ft)', ax=ax)

ax.set_xlim([30, 220])
ax.set_ylim([30, 220])
ax.plot([0, 220], [0, 220])
plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")

"""# Organize Duvernay temperature data

## Read in DST BHT input and "true temp" synthetic data
"""

Duvernay_Truth = pd.read_excel(dir_duvernay+'Duvenay TrueTemp_Train.xlsx')
Duvernay_DST = pd.read_excel(dir_duvernay+'Duvernay DST BHT for SPE April 20 2021.xlsx')
# Duvernay_Truth.head()

# add in an extra column calculating the depth sub sea (elevation-depth)*-1 
Duvernay_DST['Depth_SS(m)']=-1*(Duvernay_DST['elevation M above sea level']-(Duvernay_DST['DST Start Depth (MD) (m)']+Duvernay_DST['DST End Depth (MD) (m)'])/2)
# Duvernay_DST.head()

# merge data
Duvernay_Combined = Duvernay_DST.merge(Duvernay_Truth, on='UWI', how='left')
# Duvernay_Combined.head()

def predict_well_temp_duv(dataset):
  scores = []
  df_all_wells = []
  wells = pd.unique(dataset["UWI"])
  for well in wells:
    well_temp_depth_data = dataset.loc[well == dataset["UWI"], dataset.columns]
    if len(well_temp_depth_data) == 1:
      df = pd.DataFrame(data=None, columns = ["UWI", "True Bottomhole Depth (m)", "New_Bottomhole_TempTrue_degC"])
      df["True Bottomhole Depth (m)"] = dataset.loc[well == dataset["UWI"], "Depth_SS(m)"]
      df["UWI"] = well
      df["New_Bottomhole_TempTrue_degC"] = np.nan
      scores.append(np.nan)
    else:
      # train
      X_train, X_test, y_train, y_test = train_test_split(well_temp_depth_data["Depths subsea (m)"].values, well_temp_depth_data["True Temperature (oC)"].values, test_size=0.25, random_state=42)
      regr = RandomForestRegressor(n_estimators=10)
      regr.fit(X_train.reshape(-1, 1), y_train)
      scores.append(regr.score(X_test.reshape(-1, 1), y_test))

      # predict
      X_new = dataset.loc[well == dataset["UWI"], "Depth_SS(m)"].values
      new_temps = regr.predict(X_new.reshape(-1, 1))
      df = pd.DataFrame(data=None, columns = ["UWI", "True Bottomhole Depth (m)", "New_Bottomhole_TempTrue_degC"])
      df["True Bottomhole Depth (m)"] = dataset.loc[well == dataset["UWI"], "Depth_SS(m)"]
      df["UWI"] = well
      df["New_Bottomhole_TempTrue_degC"] = new_temps
    df_all_wells.append(df)

  df_all_wells_ = pd.concat(df_all_wells)
  return df_all_wells_, scores

Duvernay_new, model_scores = predict_well_temp_duv(Duvernay_Combined)
Duvernay_Combined_new = pd.merge(Duvernay_Combined, Duvernay_new, left_index=True, right_index=True)
Duvernay_Combined_new = Duvernay_Combined_new.drop(["Well ID", "UWI_y", "True Bottomhole Depth (m)"], axis=1)
Duvernay_Combined_new = Duvernay_Combined_new.rename(columns={"UWI_x": "UWI"})
# Duvernay_Combined_new.head()

# wells__ = pd.unique(Duvernay_Combined["UWI"])
# xxxx = Duvernay_Combined.loc[wells__[511] == Duvernay_Combined["UWI"], Duvernay_Combined.columns]
# xxxx

# keep only the synthetic model temperature data for the relevant depths for which there is BHT measurement
Duvernay_Combined_new['diff_depth'] = Duvernay_Combined_new['Depth_SS(m)']-Duvernay_Combined_new['Depths subsea (m)']
Duvernay_Combined_new['diff_depth_abs'] = np.abs(Duvernay_Combined_new['diff_depth'])
idx = Duvernay_Combined_new.groupby(['UWI'])['diff_depth_abs'].transform(min) == Duvernay_Combined_new['diff_depth_abs']

TrueTempUWI = Duvernay_Combined_new.loc[idx, ['UWI', 'diff_depth_abs', 'True Temperature (oC)', 'New_Bottomhole_TempTrue_degC']]
TrueTempUWI = TrueTempUWI.copy(deep=True)
Duvernay_Combined_cln = Duvernay_DST.merge(TrueTempUWI, on='UWI', how='left')
Duvernay_Combined_cln = Duvernay_Combined_cln.drop_duplicates(['UWI'])
# Duvernay_Combined_cln.head()

# len(Duvernay_Combined_cln)

"""## Merge in static temperature log data"""

Duvernay_Combined_cln['UWI'] = Duvernay_Combined_cln['UWI'].astype(str)
Duvernay_Combined_cln = Duvernay_Combined_cln.copy(deep=True)
Duvernay_Combined_cln['TrueTemp_datasource_syn'] = 'synthetic'
Static_log_temp['TrueTemp_datasource_stat'] = 'static_temp_logs'
Duvernay_Combined_stat = Duvernay_Combined_cln.merge(Static_log_temp, left_on='UWI',right_on='Well_ID', how='left')

# Static_log_temp.head()

# Coalesce columns together with priority for true temperature measurements
Duvernay_Combined_stat['TempC_Fin_provided'] = Duvernay_Combined_stat['Temp (degC)'].fillna(Duvernay_Combined_stat['True Temperature (oC)'])
Duvernay_Combined_stat['TempC_Fin_extrapolated'] = Duvernay_Combined_stat['Temp (degC)'].fillna(Duvernay_Combined_stat['New_Bottomhole_TempTrue_degC'])
Duvernay_Combined_stat['TrueTemp_datasource'] = Duvernay_Combined_stat['TrueTemp_datasource_stat'].fillna(Duvernay_Combined_stat['TrueTemp_datasource_syn'])
# Duvernay_Combined_stat.head()

Duvernay_Combined_stat.columns

"""## Plot the temp data"""

import matplotlib.pyplot as plt
plt.close('all')
fig, ax = plt.subplots(1, 1, figsize=(4,4))

sns.scatterplot(data=Duvernay_Combined_stat, 
                x="DST Bottom Hole Temp. (degC)",
                y="TempC_Fin_extrapolated",
                hue='diff_depth_abs', ax=ax)

#ax.set_xlim([30, 220])
#ax.set_ylim([30, 220])
ax.plot([0, 220], [0, 220])
plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")

"""# Combine Eaglebine and Duvernay temperature datasets"""

# Eaglebine_Combined.columns

Duvernay_Combined_stat['Depth_SS(ft)'] = Duvernay_Combined_stat['Depth_SS(m)'] * 3.28084                                      # convert from meters to feet
Duvernay = Duvernay_Combined_stat[['UWI', 'Depth_SS(ft)', 'DST Bottom Hole Temp. (degC)', 'Test Type', 'Formation DSTd', 
                                   'TempC_Fin_provided', 'TempC_Fin_extrapolated', 'TrueTemp_datasource']]
Duvernay = Duvernay.rename(columns={'DST Bottom Hole Temp. (degC)': 'BHT', 'TempC_Fin_provided': 'TrueTemp_provided', 'TempC_Fin_extrapolated': 'TrueTemp_extrapolated'})
Duvernay['Field'] = 'Duvernay'

Eaglebine = Eaglebine_Combined_stat[['UWI', 'TempC_BHT', 'BHT_below sea level (ft)', 'TSC or ORT (time since circulation or original recorded time in hours)', 
                                     'TempC_Fin_provided', 'TempC_Fin_extrapolated', 'TrueTemp_datasource']]
Eaglebine = Eaglebine.rename(columns={'TempC_BHT': 'BHT', 'TempC_Fin_provided': 'TrueTemp_provided', 'TempC_Fin_extrapolated': 'TrueTemp_extrapolated', 
                                      'TSC or ORT (time since circulation or original recorded time in hours)' :'Time Since Circ', 'BHT_below sea level (ft)': 'Depth_SS(ft)'})
Eaglebine['Field'] = 'Eaglebine'

combined_temperature = pd.concat((Duvernay, Eaglebine))
# combined_temperature.head()

combined_temperature.isnull().sum()
# xxx = combined_temperature['Field'] == 'Duvernay'
# xxx.sum()

# save this structured data to file
! mkdir StructuredData
combined_temperature.to_csv('StructuredData/combined_temperature_new.csv')

plt.close('all')
fig, ax = plt.subplots(1, 1, figsize=(4,4))

sns.scatterplot(data=combined_temperature, 
                x="BHT",
                y="TrueTemp_extrapolated",
                hue='Field', ax=ax)

ax.plot([0, 220], [0, 220])
plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")

"""# 5 Organize Mud-weight, formation, well header, and production data

This section consolidates the files provided by GTS. List of imported files and the dataframe names:

- Eaglebine mud weight SPE April 21 2021.xlsx: **EB_MW**
- Eaglebine formation tops SPE April 20 2021.xlsx: **EB_FT**
- Eaglebine well headers SPE April 21 2021.xlsx: **EB_WH**
- SPE Eaglebine production summary April 20 2021.xlsx: **EB_PS**
- Duvernay formation tops SPE April 20 2021.xlsx: **DV_FT**
- Duvernay well headers SPE April 21 2021 .xlsx: **DV_WH**
- SPE Duvernay production summary April 20 2021.xlsx: **DV_PS**

**Workflow: add the multiple entries as attributes**

1. read the excel file into a dataframe. Leave the indexing out at this stage.
1. for each well number (Well_Id), tag the multiple entries in a new column.
    - New column name: 'attribute_' + the name of the column with multiple entries.
    - Tag: name of the column with multiple entries + sequential number for multiple entries in that column
1. reshape the featured table to pivot on the attribute
1. check for dupblicates on 'Well_Id' after reshaping. If duplicates exist inspect all columns for each well. If no duplicates, it's ready for merge


(Thanks Korang Modaressi for this section of the code)
"""

files_to_upload = {'file': ['Eaglebine mud weight SPE April 21 2021.xlsx', 
                         'Eaglebine formation tops SPE April 20 2021.xlsx',
                         'Eaglebine well headers SPE April 21 2021.xlsx',
                         'SPE Eaglebine production summary April 20 2021.xlsx',
                         'Duvernay formation tops SPE April 20 2021.xlsx',
                         'Duvernay well headers SPE April 21 2021 .xlsx',
                         'SPE Duvernay production summary April 20 2021.xlsx'],
                  'df_name': ['EB_MW', 'EB_FT', 'EB_WH', 'EB_PS', 'DV_FT', 'DV_WH', 'DV_PS']}
files_to_upload['flat_df_name'] = [str(c) + '_flat' for c in files_to_upload['df_name']]
print(files_to_upload['flat_df_name'])
print(files_to_upload['df_name'][0][:2])

EB_MW = pd.read_excel(dir_eaglebine+"Eaglebine mud weight SPE April 21 2021.xlsx")
# display(EB_MW.head())
EB_MW.columns = [c.strip() for c in EB_MW.columns.values.tolist()]
EB_MW.rename(columns = {'UWI': 'Well_Id'}, inplace = True)
# EB_MW

EB_FT = pd.read_excel(dir_eaglebine+"Eaglebine formation tops SPE April 20 2021.xlsx")
# display(EB_FT.head())
EB_FT.columns = [c.strip() for c in EB_FT.columns.values.tolist()]
EB_FT.rename(columns = {'UWI': 'Well_Id'}, inplace = True)

EB_WH = pd.read_excel(dir_eaglebine+"Eaglebine well headers SPE April 21 2021.xlsx")
# display(EB_WH.head())
EB_WH.rename(columns = {'displayapi': 'Well_Id'}, inplace = True)

EB_PS = pd.read_excel(dir_eaglebine+"SPE Eaglebine production summary April 20 2021.xlsx")
# display(EB_PS.head())
EB_PS.columns = [c.strip() for c in EB_PS.columns.values.tolist()]
EB_PS.rename(columns = {'API': 'Well_Id'}, inplace = True)

DV_FT = pd.read_excel(dir_duvernay+"Duvernay formation tops SPE April 20 2021.xlsx")
# display(DV_FT.head())
DV_FT.columns = [c.strip() for c in DV_FT.columns.values.tolist()]
DV_FT.rename(columns = {'UWI': 'Well_Id'}, inplace = True)

DV_WH = pd.read_excel(dir_duvernay+"Duvernay well headers SPE April 21 2021 .xlsx")
# display(DV_WH.head())
DV_WH.columns = [c.strip() for c in DV_WH.columns.values.tolist()]
DV_WH.rename(columns = {'UWI': 'Well_Id'}, inplace = True)

DV_PS = pd.read_excel(dir_duvernay+"SPE Duvernay production summary April 20 2021.xlsx")
# display(DV_PS.head())
DV_PS.columns = [c.strip() for c in DV_PS.columns.values.tolist()]
DV_PS.rename(columns = {'API': 'Well_Id'}, inplace = True)

# Loop thorugh each well (Well_Id), convert each column into ndarray with Well_Id as key
# Useful functions definitions
''' list the unique well identifiers (Well_Id) in the table and the number of rows for each. The new dataframe will be used to populate the columns of the wide table'''
def wells_and_attributes(df):
    well_data = df
    columns = well_data.columns
    well_data_count = well_data['Well_Id'].value_counts()
    wells = well_data_count.index
    return well_data, wells, columns

''' 
The 'entry_to_attr' function collects the unique Well_Id and pivots entries under each Well_Id into attributes. 
The loop check for each unique well number in well_data_count, loops through columns and pivots the values in each column into a dataframe.
The name of the current column is used as prefix with an underscore "columnname_" joined with suffix as the index (from 0) of the entries for each well. 
The extracted variables are turned into dataframe and is concatenated with the existing dataframe from previous loop. 
'''
def entry_to_attr(df):
    well_data_flat = pd.DataFrame() #dataframe for storing well records after change to wide format
    well_data, wells, columns = wells_and_attributes(df) #read 'wells_and_attributes' function description
    for well in wells:
        well_data_intermediate_flat = pd.DataFrame() #temporary dataframe to store pivoted entries to new attributes for each original column
        well_data_filtered = well_data[well_data['Well_Id'] == well] #filterd for 'well' to start pivoting to new attributes
        #print(well_data_filtered)
        for column in columns: #pivots columns for each well into new attributes
            if column == 'Well_Id': #ignores the Well_Id and flattens othe columns
                continue
            else:
                att_vars = np.array(well_data_filtered[column]) #array containing entries in 'column' for 'well'
                att_count = len(att_vars) #number of entries in the array used for naming the new columns
                att_names = [column+"_"+str(x) for x in range(att_count)] #new column name with number of entries (att_count array's length) as suffix
                well_data_intermediate = pd.DataFrame(att_vars).T #intermediate flat table to be merged with the temporary dataframe
                well_data_intermediate.columns = att_names
                index = pd.Index([well], name = 'Well_Id')
                well_data_intermediate.index = index
                well_data_intermediate_flat = pd.concat([well_data_intermediate_flat, well_data_intermediate], axis = 1)
        well_data_flat = pd.concat([well_data_flat, well_data_intermediate_flat])
    return well_data_flat

'''
In this loop the entry_to_attr funciton is looped for the dataframes:
    'EB_MW', 'EB_FT', 'EB_WH', 'EB_PS', 'DV_FT', 'DV_WH', 'DV_PS'
The resulting wide-format dataframe is stored in a new datafram. 
The wide-format dataframes are concatenated into one dataframe with Well_ID as index
'''
dataframes = [EB_MW, EB_FT, EB_WH, EB_PS, DV_FT, DV_WH, DV_PS]
consolidated_well_data = pd.DataFrame()
cols = []
rows = []
# consolidated_well_data = pd.concat([consolidated_well_data, dataframes[0], dataframes[1], dataframes[2]])
for i in range(len(dataframes)): #
    well_data_flat = entry_to_attr(dataframes[i])
    well_data_flat['Basin'] = files_to_upload['df_name'][i][:2]
    well_data_flat.reset_index(inplace = True)
    well_data_flat.to_excel(str(str(files_to_upload['flat_df_name'][i]) + '.' + 'xlsx'))
    print(well_data_flat.shape)
    well_data_flat.set_index(['Well_Id', 'Basin'], inplace = True)
    consolidated_well_data = pd.concat([consolidated_well_data, well_data_flat], axis = 1)
consolidated_well_data.shape
# display(consolidated_well_data.head())
print('the consolidated_well_data has {} rows and {} columns'.format(consolidated_well_data.shape[0], consolidated_well_data.shape[1]))

consolidated_well_data_no_index = consolidated_well_data.reset_index()
# consolidated_well_data.to_csv('StructuredData/consolidated_well_data_new.csv')
# consolidated_well_data.to_excel('StructuredData/consolidated_well_data_new.xlsx')
consolidated_well_data.to_excel('consolidated_well_data_new.xlsx')

"""# Organize LAS file"""

### Thanks to Pushpesh Sharma for writing this section

import os
!pip install lasio
import lasio

#Load all files at once into las and las_df to save time
folder='clean_las_files/Clean_LAS/'
all_files = os.listdir(folder)
n_files = len(all_files)

bad_files = []

las = {}
las_df = {}
mnemonics ={}
i=0
for filename in tqdm(os.listdir(folder)):
    i=i+1
    if filename.endswith(".LAS"):
        las[filename] = lasio.read(folder+'/'+filename)
        las_df[filename] = las[filename].df()
        mnemonics[filename] = las_df[filename].columns

#find out which well curves/logs are in each las file
listDF = []
for filename in tqdm(all_files):
    df = pd.DataFrame(columns = list(mnemonics[filename]), data = np.ones((1,len(mnemonics[filename]))))
    df['well_name']=filename
    listDF.append(df)

log_table=pd.concat(listDF)

# Here we can see which logs are in each well
# log_table.head()

# see what are the most common log types
sumT = log_table.drop(columns=['well_name']).sum()
sumT.sort_values(ascending=False)

# make a table of the log types available per well
for filename in all_files:
    las_df[filename] = las_df[filename].rename_axis('DEPT').reset_index()

# we can extract the gamma ray values [GRWS] at 
# regular intervals to add to the dataset (every 300 ft)
# WellLog = 'GRWS'              # Gamma Ray
# WellLog = 'SPR'               # Resistivity log
# WellLog = 'DRESWS'            # Deep Resistivity Log
# WellLog = 'CONDWS'            # Conductivity log
# WellLog = 'NEUTED'            # Neutron Density
WellLog = 'CALWS'             # Caliper logging


select_depth = list(np.arange(12500, 13250, 0.1))
new_las_df = {}
extracted_df = {}
j = 0
fncnt = 0
for filename in tqdm(all_files):
    fncnt = fncnt+1

    las_df[filename] = las_df[filename].sort_values(by='DEPT')
    p = las_df[filename]
    new_las_df[filename] = p[p['DEPT'].isin(select_depth)]
    if(WellLog not in list(new_las_df[filename].columns)):
      continue
    q = new_las_df[filename][WellLog]
    Depth = new_las_df[filename]['DEPT']
    
    concat_list = list()
    column_name = list()
    for i in range(0,q.shape[0]):
        concat_list.append(q.iloc[i])
        # column_name.append(str(Depth.iloc[i])+'_'+WellLog)
        column_name.append(str(Depth.iloc[i]))
        
    concat_array = np.array(concat_list)
    concat_array = np.reshape(concat_array,(1,len(concat_list)))
    df = pd.DataFrame(concat_array, columns=column_name)
    df['WellName'] = filename[2:16]
    if filename[-5] == 'W':
        df['LogType'] = 'Cleaned'
    else:
        df['LogType'] = 'Raw'
    extracted_df[j] = df
    j = j+1

# save data to csv file
LargeDF = pd.concat(extracted_df)
# LargeDF.to_csv('StructuredData/LogData_new.csv')
LargeDF.to_csv('LogData_new.csv')

"""# Combine all data sources together"""

TemperatureData = pd.read_csv('StructuredData/combined_temperature_new.csv')
HeaderData = pd.read_csv('StructuredData/consolidated_well_data_new.csv')
LogData = pd.read_csv('StructuredData/LogData_new.csv')

Combined1 = TemperatureData.merge(HeaderData, how='left', left_on='UWI', right_on='Well_Id')
LogData['WellName']=LogData['WellName'].astype('str')
Combined1['UWI']=Combined1['UWI'].astype('str')
LogData = LogData.drop_duplicates(['WellName'])
Combined2 = Combined1.merge(LogData, how='left', left_on='UWI', right_on='WellName')
# Combined2.head()

Combined2.to_csv('StructuredData/Combined_all_latest.csv')
Combined2.to_excel('StructuredData/Combined_all_latest.xlsx')

print('There are ' + str(len(Combined2)) + ' rows and ' + str(len(Combined2.columns) ) + ' columns in the dataframe')

Combined2_ = Combined2.drop(['WellName', 'LogType'], axis=1)
# Combined3 = pd.DataFrame(columns = ['Gamma Ray'])
Combined3 = pd.concat([pd.DataFrame(columns = Combined2_.columns[1:8]), pd.DataFrame(columns = ['Gamma Ray'])])
for idx in range(len(Combined2)):
  BHT_depth = Combined2_['Depth_SS(ft)'][idx]
  total_number_of_columns = len(Combined2_.columns)
  log_columns = Combined2_.columns[252:]
  rem_num_columns = total_number_of_columns - len(log_columns)
  depth_diff = []
  count = 0
  for column in log_columns:
    count += 1                                     # for debugging
    depth = float(column.split("_")[0])
    depth_diff.append(abs(BHT_depth - depth))
  column_idx_ = depth_diff.index(min(depth_diff))  # idx of column with the least depth difference within log columns
  column_idx = 252 + column_idx_                   # idx of column with the least depth difference within the entire dataframe columns
  data = Combined2_.iloc[idx, column_idx]
  count_ = 5                                       # perform for maximum of 5 times
  col_idx = column_idx + 2
  while np.isnan(data):                            # if nan, use the nearest depth
    col_idx -= 1
    count_ -= 1
    data = Combined2_.iloc[idx, col_idx]
    if count_ == 0:
      break
  df = Combined2_.iloc[idx:idx+1, 1:11]
  df['Gamma Ray'] = data
  Combined3 = pd.concat([Combined3, df])
Combined3.head()

# Combined3.isnull().sum()
# Combined3.columns

"""# Master Dataset"""

# # Label encoding
# # import labelencoder
# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()

# # apply le on categorical feature columns
# Combined3[['Field_code', 'TrueTemp_datasource_code', 'Test_Type_code', 'Formation_code']] = Combined3[['Field', 'TrueTemp_datasource', 'Test Type', 'Formation DSTd']].apply(lambda col: le.fit_transform(col))
# Combined3.head()

"""# Separate into the training and validation/testing sets"""

set_split = pd.read_csv('Data for Datathon/set_assign.csv')
# set_split.head()

Combined3_ = Combined3.merge(set_split, on='UWI', how='left')

filterTraining = Combined3_['Set']=='Training'
Combined3_[filterTraining].to_csv('training_new.csv', index=False)

filterTesting = Combined3_['Set']=='Validation_Testing'
Combined3_[filterTesting].to_csv('val_data_no_label_new.csv', index=False)

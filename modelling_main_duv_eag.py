# -*- coding: utf-8 -*-
"""modelling - main_duv_eag.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BOWHxrT71OJngqPEb8UvzunkEeyYmh_K
"""
""" Author : Seyide Hunyinbo
"""
# # pandas profiling
# !pip install pandas-profiling==2.8.0

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas_profiling import ProfileReport
import pickle

import warnings
warnings.filterwarnings('ignore')

#progress bar
from tqdm.notebook import tqdm

df_train = pd.read_csv("training_new_latest.csv")
df_train.head()

df_train.isnull().sum()

df_train_ = df_train[['UWI', 'Depth_SS(ft)','BHT', 'TrueTemp_datasource_code', '0', '1', '2', '3', 'Time Since Circ', 'Mud Weight', 'Log Data Values', 'TrueTemp_provided', 'TrueTemp_extrapolated']]

# Duvernay
filter_duvernay = df_train['Field'] == 'Duvernay'
df_train_duvernay = df_train_[filter_duvernay]
df_train_duvernay.set_index('UWI', inplace=True)
df_train_duvernay = df_train_duvernay.drop(columns=['Time Since Circ', 'Mud Weight', 'Log Data Values'])
df_train_duvernay.head()

# Eaglebine
filter_eaglebine = df_train['Field'] == 'Eaglebine'
df_train_eaglebine = df_train_[filter_eaglebine]
df_train_eaglebine.set_index('UWI', inplace=True)
df_train_eaglebine = df_train_eaglebine.drop(columns=['Log Data Values'])
df_train_eaglebine = df_train_eaglebine.dropna()
df_train_eaglebine.head()

# df_train_duvernay.isnull().sum()
print(df_train_eaglebine.info())
print('\n')
print(f'Proportion of static_temp_logs (Duvernay): {df_train_duvernay["2"].sum() / len(df_train_duvernay) * 100} %')
print(f'Proportion of static_temp_logs (Eaglebine): {df_train_eaglebine["2"].sum() / len(df_train_eaglebine) * 100} %')

"""# Exploratory Data Analysis"""

# # Duvernay
# profile = ProfileReport(df_train_duvernay[['Depth_SS(ft)','BHT', 'TrueTemp_provided', 'TrueTemp_extrapolated']], title="Data")
# profile.to_notebook_iframe()

# # Eaglebine
# profile = ProfileReport(df_train_eaglebine[['Depth_SS(ft)','BHT', 'Time Since Circ', 'Mud Weight', 'TrueTemp_provided', 'TrueTemp_extrapolated']], title="Data")
# profile.to_notebook_iframe()

"""# Feature Selection"""

# Eaglebine
X_eaglebine = df_train_eaglebine[['Depth_SS(ft)','BHT', 'TrueTemp_datasource_code', 'Time Since Circ', 'Mud Weight']].values

# Duvervay
X_duvernay = df_train_duvernay[['Depth_SS(ft)', 'TrueTemp_datasource_code', 'BHT']].values
print(X_duvernay[:5,:])

y = df_train['TrueTemp_provided'].values
print(y[:5])

# Eaglebine
y_eaglebine = df_train_eaglebine['TrueTemp_extrapolated'].values
print(y_eaglebine[:5])

# Duvervay
y_duvervay = df_train_duvernay['TrueTemp_extrapolated'].values
print(y_duvervay[:5])

"""# Train/Test Split and Outlier Detection"""

# Perform Train-Test Split
from sklearn.model_selection import train_test_split
# Eaglebine
X_train_eaglebine, X_test_eaglebine, y_train_eaglebine, y_test_eaglebine = train_test_split(X_eaglebine, y_eaglebine, test_size=0.1, random_state=42)
print(X_train_eaglebine.shape, X_test_eaglebine.shape)

# Duvervay
X_train_duvernay, X_test_duvernay, y_train_duvernay, y_test_duvernay = train_test_split(X_duvernay, y_duvervay, test_size=0.1, random_state=42)
print(X_train_duvernay.shape, X_test_duvernay.shape)

# Outlier Detection
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM

from random import seed
seed(42)

# # Isolation Forest
# iso = IsolationForest(contamination=0.1)
# yhat = iso.fit_predict(X_train)

# iso_duv = IsolationForest(contamination=0.1)
# yhat_duvernay = iso_duv.fit_predict(X_train_duvernay)

# iso_eag = IsolationForest(contamination=0.1)
# yhat_eaglebine = iso_eag.fit_predict(X_train_eaglebine)

# # # Minimum Covariance Determinant (MCD) method 
# # ee = EllipticEnvelope(contamination=0.01)
# # yhat = ee.fit_predict(X_train)

# # # Local Outlier Factor
# # lof = LocalOutlierFactor()
# # yhat = lof.fit_predict(X_train)

# # # One Class SVM
# # ee = OneClassSVM(nu=0.01)
# # yhat = ee.fit_predict(X_train)

# # select all rows that are not outliers
# mask = yhat != -1
# mask_duvernay = yhat_duvernay != -1
# mask_eaglebine = yhat_eaglebine != -1
# X_train, y_train = X_train[mask, :], y_train[mask]
# X_train_duvernay, y_train_duvernay = X_train_duvernay[mask_duvernay, :], y_train_duvernay[mask_duvernay]
# X_train_eaglebine, y_train_eaglebine = X_train_eaglebine[mask_eaglebine, :], y_train_eaglebine[mask_eaglebine]

# summarize the shape of the updated training dataset
print('\n')
print(X_train_eaglebine.shape, X_test_eaglebine.shape)
print(X_train_duvernay.shape, X_test_duvernay.shape)

# Standardization/Normalization
from sklearn.preprocessing import StandardScaler, MinMaxScaler
# # # Duvernay
# scaler_duvernay = StandardScaler().fit(X_train_duvernay[:,:2])
# normalizer_duvernay = MinMaxScaler().fit(X_train_duvernay[:,:2])

# X_train_duvernay_standardized = scaler_duvernay.transform(X_train_duvernay[:,:2])
# X_train_duvernay_normalized = normalizer_duvernay.transform(X_train_duvernay[:,:2])

# X_test_duvernay_standardized = scaler_duvernay.transform(X_test_duvernay[:,:2])
# X_test_duvernay_normalized = normalizer_duvernay.transform(X_test_duvernay[:,:2])

# X_train_duvernay[:,:2] = X_train_duvernay_standardized
# X_test_duvernay[:,:2] = X_test_duvernay_standardized

# X_train_duvernay[:,:2] = X_train_duvernay_normalized
# X_test_duvernay[:,:2] = X_test_duvernay_normalized


# # Eaglebine
# scaler_eaglebine = StandardScaler().fit(X_train_eaglebine[:,:2])
# normalizer_eaglebine = MinMaxScaler().fit(X_train_eaglebine[:,:4])

# X_train_eaglebine_standardized = scaler_eaglebine.transform(X_train_eaglebine[:,:2])
# X_train_eaglebine_normalized = normalizer_eaglebine.transform(X_train_eaglebine[:,:4])

# X_test_eaglebine_standardized = scaler_eaglebine.transform(X_test_eaglebine[:,:2])
# X_test_eaglebine_normalized = normalizer_eaglebine.transform(X_test_eaglebine[:,:4])

# X_train_eaglebine[:,:2] = X_train_eaglebine_standardized
# X_test_eaglebine[:,:2] = X_test_eaglebine_standardized

# X_train_eaglebine[:,:4] = X_train_eaglebine_normalized
# X_test_eaglebine[:,:4] = X_test_eaglebine_normalized

from sklearn.linear_model import Ridge, LinearRegression, SGDRegressor, RidgeCV, LassoCV, ElasticNetCV
from sklearn.model_selection import RepeatedKFold, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error

"""# Hyper-parameter optmization for linear model"""

# define space
space = dict()

regr = Ridge(random_state=42)
space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']
space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
space['normalize'] = [True, False]
space['fit_intercept'] = [True, False]

cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)

# define search
search = GridSearchCV(regr, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)
# execute search
result = search.fit(X_train_duvernay, y_train_duvernay)
# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

"""# Linear Regression - Duvernay"""

regr_duv = Ridge(alpha=1e-05, fit_intercept=True, normalize=False, solver='svd', random_state=42)
# regr_duv = Ridge(alpha=100, fit_intercept=True, normalize=False, solver='svd', random_state=42)
regr_duv.fit(X_train_duvernay, y_train_duvernay)

# Model evaluation
from sklearn.metrics import r2_score, mean_absolute_error
y_predict_lr_train = regr_duv.predict(X_train_duvernay)
y_predict_lr = regr_duv.predict(X_test_duvernay)

print("Train MAE: %.2f" % mean_absolute_error(y_train_duvernay, y_predict_lr_train))
print("Train R2-score: %.2f" % r2_score(y_train_duvernay, y_predict_lr_train))
print('\n')
print("Test MAE: %.2f" % mean_absolute_error(y_test_duvernay, y_predict_lr))
print("Test R2-score: %.2f" % r2_score(y_test_duvernay, y_predict_lr))

fig, ax = plt.subplots()
sns.scatterplot(y_train_duvernay, y_predict_lr_train, ax=ax)
sns.scatterplot(y_test_duvernay, y_predict_lr, ax=ax)
sns.lineplot(y_train_duvernay, y_train_duvernay, ax=ax)
plt.xlabel("y_test")
plt.ylabel("y_predict")
plt.show()

# Create testset dataframe
df_test = pd.DataFrame(X_test_duvernay, columns = ['Depth_SS(ft)', 'TrueTemp_datasource_code', 'BHT'])

for data in range(X_test_duvernay.shape[0]):
  if X_test_duvernay[data,1] == 1:
    df_test.loc[data, 'TrueTemp_datasource_code'] = 'synthetic'
  elif X_test_duvernay[data,1] == 0:
    df_test.loc[data, 'TrueTemp_datasource_code'] = 'static_well_logs'

df_test['true_temps_actual'] = y_test_duvernay
df_test['true_temp_predicted'] = y_predict_lr

# Visualize
fig, ax = plt.subplots()

sns.scatterplot(data= df_test, x="true_temps_actual", y="true_temp_predicted", hue='TrueTemp_datasource_code', ax=ax)
ax.set_title('Temperature Source')
ax.legend(loc="upper left")

plt.show()

# save linear model as a pickle file
with open("regr_duv.pkl", "wb") as outfile:
  pickle.dump(regr_duv, outfile)

"""# Linear Regression - Eaglebine"""

# define space
space = dict()

regr = Ridge(random_state=42)
space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']
space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
space['normalize'] = [True, False]
space['fit_intercept'] = [True, False]

cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)

# define search
search = GridSearchCV(regr, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)
# execute search
result = search.fit(X_train_eaglebine, y_train_eaglebine)
# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

regr_eag = Ridge(alpha=10, fit_intercept=True, normalize=False, solver='svd', random_state=42)
regr_eag.fit(X_train_eaglebine, y_train_eaglebine)

y_predict_lr_train = regr_eag.predict(X_train_eaglebine)
y_predict_lr = regr_eag.predict(X_test_eaglebine)

print("Train MAE: %.2f" % mean_absolute_error(y_train_eaglebine, y_predict_lr_train))
print("Train R2-score: %.2f" % r2_score(y_train_eaglebine, y_predict_lr_train))
print('\n')
print("Test MAE: %.2f" % mean_absolute_error(y_test_eaglebine, y_predict_lr))
print("Test R2-score: %.2f" % r2_score(y_test_eaglebine, y_predict_lr))

fig, ax = plt.subplots()
sns.scatterplot(y_train_eaglebine, y_predict_lr_train, ax=ax)
sns.scatterplot(y_test_eaglebine, y_predict_lr, ax=ax)
sns.lineplot(y_train_eaglebine, y_train_eaglebine, ax=ax)
plt.xlabel("y_test")
plt.ylabel("y_predict")
plt.show()

# Create testset dataframe
df_test = pd.DataFrame(X_test_eaglebine, columns = ['Depth_SS(ft)','BHT', 'TrueTemp_datasource_code', 'Time Since Circ', 'Mud Weight'])

for data in range(X_test_eaglebine.shape[0]):
  if X_test_eaglebine[data,2] == 1:
    df_test.loc[data, 'TrueTemp_datasource_code'] = 'synthetic'
  elif X_test_eaglebine[data,2] == 0:
    df_test.loc[data, 'TrueTemp_datasource_code'] = 'static_well_logs'

df_test['true_temps_actual'] = y_test_eaglebine
df_test['true_temp_predicted'] = y_predict_lr

# Visualize
fig, ax = plt.subplots()
sns.scatterplot(data= df_test, x="true_temps_actual", y="true_temp_predicted", hue='TrueTemp_datasource_code', ax=ax)
ax.set_title('Temperature Source')
ax.legend(loc="upper left")

plt.show()

# save linear model as a pickle file
with open("regr_eag.pkl", "wb") as outfile:
  pickle.dump(regr_eag, outfile)

"""# Ensemble methods"""

from sklearn.ensemble import RandomForestRegressor
from xgboost.sklearn import XGBRegressor
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error

"""# XG Boost Duvernay"""

# Create the random grid
random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],
               'max_features': ['auto', 'sqrt'],
               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],
               'min_samples_split': [2, 5, 10],
               'min_samples_leaf': [1, 2, 4]}

xgb_duv = XGBRegressor(random_state=42)
xgb_random_duv = RandomizedSearchCV(estimator = xgb_duv, param_distributions = random_grid, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs=-1)
xgb_random_duv.fit(X_train_duvernay, y_train_duvernay)

xgb_tuned_duv = xgb_random_duv.best_estimator_
xgb_random_duv.best_params_

# Fit model
eval_set = [(X_train_duvernay, y_train_duvernay), (X_test_duvernay, y_test_duvernay)]
xgb_tuned_duv.fit(X_train_duvernay, y_train_duvernay, early_stopping_rounds=10, eval_metric = 'mae', eval_set = eval_set, verbose=False)

# retrieve performance metrics
results = xgb_tuned_duv.evals_result()
epochs = len(results['validation_0']['mae'])
x_axis = range(0, epochs)

# plot learning curve
fig, ax = plt.subplots()
ax.plot(x_axis, results['validation_0']['mae'], label='Train')
ax.plot(x_axis, results['validation_1']['mae'], label='Test')
ax.legend()
plt.ylabel('MAE')
plt.title('XGBoost Learning Curve')
plt.show()

y_predict_xgb_train = xgb_tuned_duv.predict(X_train_duvernay)
y_predict_xgb = xgb_tuned_duv.predict(X_test_duvernay)

print("Train MAE: %.2f" % mean_absolute_error(y_train_duvernay, y_predict_xgb_train))
print("Train R2-score: %.2f" % r2_score(y_train_duvernay, y_predict_xgb_train))
print('\n')
print("Test MAE: %.2f" % mean_absolute_error(y_test_duvernay, y_predict_xgb))
print("Test R2-score: %.2f" % r2_score(y_test_duvernay, y_predict_xgb))

fig, ax = plt.subplots()
sns.scatterplot(y_train_duvernay, y_predict_xgb_train, ax=ax)
sns.scatterplot(y_test_duvernay, y_predict_xgb, ax=ax)
sns.lineplot(y_train_duvernay, y_train_duvernay, ax=ax)
plt.xlabel("y_test")
plt.ylabel("y_predict")
plt.show()

# save xgboost model as a pickle file
with open("xgb_tuned_duv.pkl", "wb") as outfile:
  pickle.dump(xgb_tuned_duv, outfile)

"""# XG Boost Eaglebine"""

# Create the random grid
random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],
               'max_features': ['auto', 'sqrt'],
               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],
               'min_samples_split': [2, 5, 10],
               'min_samples_leaf': [1, 2, 4]}

xgb_eag = XGBRegressor(random_state=42)
xgb_random_eag = RandomizedSearchCV(estimator = xgb_eag, param_distributions = random_grid, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs=-1)
xgb_random_eag.fit(X_train_eaglebine, y_train_eaglebine)

xgb_tuned_eag = xgb_random_eag.best_estimator_
xgb_random_eag.best_params_

# Fit model
eval_set = [(X_train_eaglebine, y_train_eaglebine), (X_test_eaglebine, y_test_eaglebine)]
xgb_tuned_eag.fit(X_train_eaglebine, y_train_eaglebine, early_stopping_rounds=10, eval_metric = 'mae', eval_set = eval_set, verbose=False)

# retrieve performance metrics
results = xgb_tuned_eag.evals_result()
epochs = len(results['validation_0']['mae'])
x_axis = range(0, epochs)

# plot learning curve
fig, ax = plt.subplots()
ax.plot(x_axis, results['validation_0']['mae'], label='Train')
ax.plot(x_axis, results['validation_1']['mae'], label='Test')
ax.legend()
plt.ylabel('MAE')
plt.title('XGBoost Learning Curve')
plt.show()

y_predict_xgb_train = xgb_tuned_eag.predict(X_train_eaglebine)
y_predict_xgb = xgb_tuned_eag.predict(X_test_eaglebine)

print("Train MAE: %.2f" % mean_absolute_error(y_train_eaglebine, y_predict_xgb_train))
print("Train R2-score: %.2f" % r2_score(y_train_eaglebine, y_predict_xgb_train))
print('\n')
print("Test MAE: %.2f" % mean_absolute_error(y_test_eaglebine, y_predict_xgb))
print("Test R2-score: %.2f" % r2_score(y_test_eaglebine, y_predict_xgb))

fig, ax = plt.subplots()
sns.scatterplot(y_train_eaglebine, y_predict_xgb_train, ax=ax)
sns.scatterplot(y_test_eaglebine, y_predict_xgb, ax=ax)
sns.lineplot(y_train_eaglebine, y_train_eaglebine, ax=ax)
plt.xlabel("y_test")
plt.ylabel("y_predict")
plt.show()

# save xgboost model as a pickle file
with open("xgb_tuned_eag.pkl", "wb") as outfile:
  pickle.dump(xgb_tuned_eag, outfile)

"""# Prediction - Duvernay"""

# Trying the validation set
df_val = pd.read_csv("val_data_no_label_new_latest_filled_missing_MW.csv")
df_val_ = df_val[['UWI', 'Depth_SS(ft)', 'TrueTemp_datasource_code', 'BHT', 'Time Since Circ', 'Mud Weight', 'Log Data Values', 'TrueTemp_provided', 'TrueTemp_extrapolated', '0', '1', '2', '3']]

# Duvernay
filter_duvernay = df_val['Field'] == 'Duvernay'
df_val_duvernay = df_val_[filter_duvernay]
df_val_duvernay.set_index('UWI', inplace=True)
df_val_duvernay = df_val_duvernay.drop(columns=['Time Since Circ', 'Mud Weight', 'Log Data Values', '0', '1', '2', '3'])
df_val_duvernay.head()

# Duvervay
X_val_duvernay = df_val_duvernay[['Depth_SS(ft)', 'TrueTemp_datasource_code', 'BHT']].values
print(X_val_duvernay[:5,:])

#Normalization / Standardization
# Duvervay
# X_val_standardized = scaler_duvernay.transform(X_val_duvernay[:,:2])
# X_val_normalized = normalizer_duvernay.transform(X_val_duvernay[:,:2])

# X_val_duvernay[:,:2] = X_val_standardized
# X_val_duvernay[:,:2] = X_val_normalized
# X_val_duvernay.std(axis=0)

val_prediction = regr_duv.predict(X_val_duvernay)      # linear regression
# val_prediction = rf_duv.predict(X_val_duvernay)        # ensemble
# val_prediction = xgb_tuned_duv.predict(X_val_duvernay)        # ensemble

# to converting column to dataframe 
df_val_prediction = pd.DataFrame(val_prediction, columns=['TrueTemp_val'])
print(df_val_prediction)
print('\n')
df_val_duvernay['TrueTemp'] = val_prediction
df_val_duvernay.head()

"""# Prediction - Eaglebine"""

# Eaglebine
filter_eaglebine = df_val['Field'] == 'Eaglebine'
df_val_eaglebine = df_val_[filter_eaglebine]
df_val_eaglebine.set_index('UWI', inplace=True)
df_val_eaglebine = df_val_eaglebine.drop(columns=['Log Data Values'])
# df_train_eaglebine = df_val_eaglebine.drop(columns=['Time Since Circ', 'Log Data Values'])
df_val_eaglebine.head()

# Eaglebine
X_val_eaglebine = df_val_eaglebine[['Depth_SS(ft)','BHT', 'TrueTemp_datasource_code', 'Time Since Circ', 'Mud Weight']].values
print(X_val_eaglebine[:5,:])

#Normalization / Standardization
# Eaglebine
# X_val_standardized = scaler_eaglebine.transform(X_val_eaglebine[:,:4])
# X_val_normalized = normalizer_eaglebine.transform(X_val_eaglebine[:,:4])

# X_val_eaglebine[:,:4] = X_val_standardized
# X_val_eaglebine[:,:4] = X_val_normalized
# X_val_eaglebine.std(axis=0)

val_prediction = regr_eag.predict(X_val_eaglebine)      # linear regression
# val_prediction = rf_eag.predict(X_val_eaglebine)        # ensemble
# val_prediction = xgb_tuned_eag.predict(X_val_eaglebine)        # ensemble

# to converting column to dataframe 
df_val_prediction = pd.DataFrame(val_prediction, columns=['TrueTemp_val'])
print(df_val_prediction)
print('\n')
df_val_eaglebine['TrueTemp'] = val_prediction
df_val_eaglebine.head()

df_val = pd.concat([df_val_duvernay, df_val_eaglebine], axis=0)
df_val.reset_index(inplace=True)
df_val.head()

df_val[['UWI','TrueTemp']].to_csv('predictions.csv')

import zipfile
zipfile.ZipFile('predictions.zip', mode='w').write("predictions.csv")